{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classifier\n",
    "In this lab, you will implement and assess the performance of the Bayesian Classifier.\n",
    "\n",
    "## Lab Instructions:\n",
    "1. Read the explanation above each requirement very well\n",
    "2. Read the requirement very well before jumping into the code.\n",
    "3. Some requirements have essay questions in them, make sure you do NOT miss them.\n",
    "4. PLEASE Read the hints! They are clear and made to help you complete the requirement as fast as you should "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### always keep all your imports in the first cell ####\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "In this requirement, you will build the Bayesian Classifier and test its performance. \n",
    "\n",
    "You are provided with a data file **data1.csv** containing list of points and their corresponding classes. The format of the data files is shown in the table below.\n",
    "\n",
    "| |Class|Feature 1|Feature 1| \n",
    "|-|-|-|-|\n",
    "|Point#1|1|0.271633|-2.93224|\n",
    "|Point#2|1|7.020786|-1.98966|\n",
    "|Point#3|1|2.901827|-0.91291|\n",
    "\n",
    "\n",
    "You are also provided with a test data file **test_data.csv**. The file contains test points that are unlabelled (i.e. the class to which they belong is unknown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [1] : Read the file 'data1.csv' into the variable data.\n",
    "# data contains the training data together with labelled classes.\n",
    "def read_data(file_name):\n",
    "    ## HINT 1: How is the data ordered in the file?\n",
    "    ## HINT 2: Do you need to cast the data you read from the file?\n",
    "    # data = pd.read_csv(file_name)\n",
    "    rows = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        csvreader = csv.reader(file, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "\n",
    "    data = np.zeros((len(rows),len(rows[0])))\n",
    "    for i in range(len(rows)):\n",
    "       data[i] = np.asarray([float(x) for x in rows[i]])\n",
    "    return data\n",
    "\n",
    "# print(read_data(\"test_data.csv\")[0][0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data():\n",
    "    \n",
    "    # TODO [2.A]: Read the file 'test_data.csv' into the variable test_data\n",
    "    # test_data contains the unlabelled test class.\n",
    "    ## HINT: Do you need to cast the data you read from the file?\n",
    "\n",
    "    test_data = read_data(\"test_data.csv\")\n",
    "    \n",
    "    # TODO [2.B]: Read the file 'test_data_true.csv' into the variable test_data_true\n",
    "    # test_data_true contains the actual classes of the test instances, which you will compare\n",
    "    # against your predicted classes.\n",
    "    ## HINT: Do you need to cast the data you read from the file?\n",
    "\n",
    "    test_data_true = read_data(\"test_data_true.csv\")\n",
    "    return test_data, test_data_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Terminlology\n",
    "Machine learning problems use common termonology (names and notiations) to refer to certain things. It is useful to use this termonology throughout your code to make it readable.\n",
    "\n",
    "| | |\n",
    "|:-|:--- |\n",
    "|$M$:|A scalar; represents the number of training points in the training set.|\n",
    "|$K$:|A scalar; represents the number of test points in the test set.|\n",
    "|$N$:|A scalar; represents the number of features of training set/test set (dimensionality of data).|\n",
    "|$X$:|A numpy array of shape $(M \\times N)$ containing the training data **without** its labels, where $M$ is the number of training points and $N$ is the number of features in the dataset (or dimensionality of features). <br/> Each element in $X$ is a tuple $(X_1, X_2, \\dots, X_N)$ where $N$ is the number of features in the dataset.| \n",
    "|$X_{test}$:| A numpy array of shape $(K \\times N)$ containing the test data, where $K$ is the number of test points and $N$ is the number of features in the dataset (or dimensionality of features). <br/> Each element in $X_{test}$ is a tuple $(X_1, X_2, \\dots, X_N)$ where $N$ is the number of features in the dataset. <br/> The number of columns in $X_{test}$ is equal to the number of columns in $X$ (because they have the same number of features). However, the number of rows in $X_{test}$ is different to the number of rows in $X$.|\n",
    "|**$Y$:| A numpy array of shape $(M \\times 1)$ containing the labels of the training data. Each row in $Y$ corresponds to the label of the training point in $X$.<br/> For example, $Y[j]$ corresponds to the label of the training point $X[j]$ where $0<=j<M$.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [3]: Fill the values of M, K, N, X, XTest, and Y respectively.\n",
    "# Do not fill them manually (i.e. do not set N = 3). They should be generic for any input file.  \n",
    "training_data = read_data('data1.csv')\n",
    "test_data, test_data_true = read_test_data()\n",
    "\n",
    "numClasses = 3 \n",
    "M = training_data.shape[0]\n",
    "N = training_data.shape[1] - 1\n",
    "K = test_data.shape[0]\n",
    "\n",
    "X = np.zeros((M,N))\n",
    "for i in range(0, int(M)):\n",
    "    X[i] = (training_data[int(i)][1:(N+1)])\n",
    "\n",
    "X_Test = np.zeros((K,N))\n",
    "for i in range(0, int(K)):\n",
    "    X_Test[i] = (test_data[int(i)][1:(N+1)])\n",
    "\n",
    "Y = np.zeros((M,1))\n",
    "for i in range(0, M):\n",
    "    Y[i] = (training_data[int(i)][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [4]: Draw a scatter plot for traning data, where each class is coloured by the colour corresponding \n",
    "#           to its index in the colors array.\n",
    "# Class 1 should be coloured in red, Class 2 should be coloured in green, and Class 3 should be coloured in blue.\n",
    "# Hint: We have done a similar plot in the previous lab. What operation do we need to select training data \n",
    "#       belonging to a certain class?\n",
    "\n",
    "colors = ['r', 'g', 'b', 'c', 'y']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "ax.scatter(X[np.where(Y == 1.0),0],X[np.where(Y == 1.0),1], color = colors[0], label = \"class 1\")\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "ax.scatter(X[np.where(Y == 2.0),0],X[np.where(Y == 2.0),1], color = colors[1], label = \"class 2\")\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "ax.scatter(X[np.where(Y == 3.0),0],X[np.where(Y == 3.0),1], color = colors[2], label = \"class 3\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "ax.set_title('Training Data')\n",
    "ax.set_xlabel('class/1')\n",
    "ax.set_ylabel('class/2')\n",
    "ax.set_zlabel('class/3')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "unique = list(set(labels))\n",
    "handles = [handles[labels.index(u)] for u in unique]\n",
    "labels = [labels[labels.index(u)] for u in unique]\n",
    "\n",
    "ax.legend(handles, labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Your Answer:\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What do you notice about the plot? (Write everything you can think of)\n",
    "'''\n",
    "    Your Answer:\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Classifier\n",
    "The Bayesian Classifier calculates the probability of the test point belonging to each class, then the class with highest probability is assigned to the test point.\n",
    "\n",
    "Classification of $x_{test}$ = $argmax_{i} P\\big(C_i|x_{test}\\big)$ = $argmax_{i} P(x|C_i) * P(C_i)$\n",
    "\n",
    "* $P(C_i|x_{test})$: Posterior probability\n",
    "* $P(x|C_i)$: Class-conditional probability (or distribution)\n",
    "* $P(C_i)$: Class apriori probability\n",
    "                \n",
    "**Note that** $P(C_i|x_{test}) \\neq P(x_{test}|C_i) * P(C_i)$. Instead,  $P(C_i|x_{test}) = \\frac{P(x_{test}|C_i) * P(C_i)}{P(x_{test})}$. However, when we compare multiple classes, the denominator $P(x_{test})$ is independent of the class $i$ and can be regarded as normalizing factor.\n",
    "\n",
    "**We start by** computing statistical parameters about each class from the data. \n",
    "\n",
    "For each class, we are interested in **three** parameters that will be used for calculating the Gaussian class-conditional distribution and the posterior probability.\n",
    "\n",
    "These parameters are:\n",
    "\n",
    "|||\n",
    "|:-|:-|\n",
    "|**Class Apriori Probability: ($P_C$)**| A scalar; the probability of class occurence (how frequent this class appears in the training data)|\n",
    "|**Class Mean: ($\\mu$)**| A vector of shape $(N \\times 1)$, it is the expected value (mean) calculated from the training points of each class.|\n",
    "|**Class Covariance Matrix: ($\\Sigma$)**| A square symmetric matrix of shape $(N \\times N)$ representing the covariances between all the feature calculated from the training points of the class. <br/> For example: Matrix element $\\sigma^2_{12}$ is the covariance between the 1st and the 2nd features|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pClasses = [] # A list of size (numClasses, 1) containing the a priori probabilities of each class in the training set.\n",
    "\n",
    "estimate_means = [] # A numpy array of size (numClasses, N) containing the mean points of each class in the training set. \n",
    "                    # HINT: USE NP.MEAN\n",
    "\n",
    "estimate_covariances = [] # A numpy array of size (numClasses, N, N) containing the covariance matrices of each class in the training set.\n",
    "                          # HINT: USE NP.COV (Pay attenention for what it takes as an argument)\n",
    "                             \n",
    "for classIndex in range(numClasses):\n",
    "    # TODO [5]: Estimate the parameters of the Gaussian distributions of the given classes.\n",
    "    # Fill pClasses, estimate_means, and estimate_covariances in this part \n",
    "    # Your code should be vectorized WITHOUT USING A SINGLE FOR LOOP.\n",
    "    pClasses.append(np.count_nonzero(Y == (1.0 + classIndex)) / Y.shape[0])\n",
    "    class_array = np.where(Y == (1.0 + classIndex))\n",
    "    class_array = np.array(class_array) # here it out 2D array first array it what we want second it zeros and i don't understand why\n",
    "    estimate_means.append(np.mean(X[class_array[0]],axis =0))\n",
    "    estimate_covariances.append(np.cov(np.subtract(X, estimate_means[classIndex].T)).T)\n",
    "    \n",
    "\n",
    "estimate_means = np.array(estimate_means)\n",
    "estimate_covariances = np.array(estimate_covariances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2.71330907 -12.92317545   0.07063062 ...  -9.27226695  -7.137776\n",
      "   -7.24111305]\n",
      " [  0.79995725  -6.37664458   0.56037856 ...  -5.72451656  12.22960125\n",
      "    6.56890625]]\n"
     ]
    }
   ],
   "source": [
    "print(np.subtract(X, estimate_means[classIndex].T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Sum of apriori probabilities should be 1, found 0.9999999999999999",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-e130636216f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;34m'Incorrect class apriori probability list, it should be of length {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Sum of apriori probabilities should be 1, found {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpClasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Sum of apriori probabilities should be 1, found 0.9999999999999999"
     ]
    }
   ],
   "source": [
    "### Test your implementation ###\n",
    "### DO NOT CHANGE THIS CODE ###\n",
    "assert len(pClasses) == numClasses,\\\n",
    "        'Incorrect class apriori probability list, it should be of length {}'.format(len(pClasses))\n",
    "assert np.sum(pClasses)==1,\\\n",
    "        'Sum of apriori probabilities should be 1, found {}'.format(np.sum(pClasses))\n",
    "        \n",
    "\n",
    "assert estimate_means.shape == (numClasses, N),\\\n",
    "        'Incorrect estimated means, it should be of shape {}'.format((numClasses, N))\n",
    "assert estimate_covariances.shape == (numClasses, N, N),\\\n",
    "        'Incorrect estimate covariance matrices, it should be of shape {}'.format((numClasses, N, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The second step** in the classifier is to calculate the class-conditional density using the Gaussian destribution:\n",
    "\n",
    "$P(x|C_i) = \\mathcal{N}(x; \\mu_i, \\Sigma_i) = \\frac{1}{(2\\pi)^{\\frac{N}{2}}|\\Sigma_i|^{\\frac{1}{2}}} exp\\big(\\frac{-1}{2}(x-\\mu_i)^T\\Sigma^{-1}_{i}(x-\\mu_i)\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Implement the multivariate normal gaussian distribution with parameters mu and sigma, and return the\n",
    "#  value in prob.\n",
    "def multivariate_normal_gaussian(X, mu, sigma):\n",
    "    prob = (np.exp(-0.5 * np.matmul(np.matmul(np.transpose(X - mu), np.linalg.pinv(sigma)), (X - mu))))/((2 * np.pi) ** (len(X)/2) * np.linalg.det(sigma)**0.5)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Test your implementation ###\n",
    "### DO NOT CHANGE THIS CODE ###\n",
    "np.random.seed(90)\n",
    "assertion_x = np.random.rand(3).reshape(-1,1)\n",
    "assertion_mu = np.random.rand(3).reshape(-1,1)\n",
    "assertion_sigma = np.random.rand(9).reshape(3,3)\n",
    "assertion_probability = multivariate_normal_gaussian(assertion_x, assertion_mu, assertion_sigma)[0][0]\n",
    "assertion_probability = round(assertion_probability, 1)\n",
    "\n",
    "assert assertion_probability == 7.8,\\\n",
    "    'Incorrect Gaussian Probability calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final step** is to go for each test point, calculate its posterior probability against each class, then classify it to the class with the highest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test point: [3.87253627 3.87253627]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1150 is different from 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-0bfb3794b8d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# TODO [7.A]: Compute the probability that the test point X_Test[i] belongs to each class in numClasses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#  Fill the array classProbabilities accordingly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mclassProbabilities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultivariate_normal_gaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimate_means\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimate_covariances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# classProbabilities[1] = multivariate_normal_gaussian(X_Test[i], estimate_means[1], estimate_covariances[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# classProbabilities[2] = multivariate_normal_gaussian(X_Test[i], estimate_means[2], estimate_covariances[2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-2a82b12ccbde>\u001b[0m in \u001b[0;36mmultivariate_normal_gaussian\u001b[1;34m(X, mu, sigma)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#  value in prob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmultivariate_normal_gaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1150 is different from 2)"
     ]
    }
   ],
   "source": [
    "# TODO [7]: Apply the Bayesian Classifier to predict the classes of the test points.\n",
    "predicted_classes = [] # predicted_classes: A numpy array of size (K, 1) where K is the number of points in the test set. Every element in this array\n",
    "                       # contains the predicted class of Bayes classifier for this test point.\n",
    "\n",
    "for i in range(X_Test.shape[0]):\n",
    "    print(\"For test point:\", X_Test[i])\n",
    "    classProbabilities = np.zeros(numClasses)\n",
    "    # TODO [7.A]: Compute the probability that the test point X_Test[i] belongs to each class in numClasses.\n",
    "    #  Fill the array classProbabilities accordingly.\n",
    "    classProbabilities[0] = multivariate_normal_gaussian(X_Test[i], estimate_means[0], estimate_covariances[0])\n",
    "    # classProbabilities[1] = multivariate_normal_gaussian(X_Test[i], estimate_means[1], estimate_covariances[1])\n",
    "    # classProbabilities[2] = multivariate_normal_gaussian(X_Test[i], estimate_means[2], estimate_covariances[2])\n",
    "    \n",
    "    # TODO [7.B]: Find the prediction of the test point X_Test[i] and append it to the predicted_classes array.\n",
    "    predicted_classes.append(np.amin(classProbabilities))\n",
    "\n",
    "    print('Class Probabilities: ', classProbabilities)  # the first class is the left most in the scatter plot\n",
    "    print(\"Predicted class is :\", predicted_classes[i])\n",
    "    print(\"******************************************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [8]: Compute the accuracy of the generated Bayesian classifier \n",
    "# WITHOUT USING ANY FOR LOOPs.\n",
    "accuracy = 0\n",
    "print('Accuracy = ' + str(round(accuracy,4) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO [9]: Generate a 3D-plot for the generated distributions. x-axis and y-axis represent the features of the data, \n",
    "#           where z-axis represent the Gaussian probability N at this point.\n",
    "\n",
    "x = np.linspace(-10, 10, 300)\n",
    "y = np.linspace(-10, 15, 300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros(X.shape)\n",
    "\n",
    "for i in range(Z.shape[0]):\n",
    "    for j in range(Z.shape[1]):\n",
    "        # TODO [9]: Fill in the matrix Z which will represent the probability distribution of every point.\n",
    "        # Z[i,j] represents the joint probability N(x,y) for x = i and y = j. \n",
    "        # We want to draw the gaussian probability N(x,y) for all points. \n",
    "        Z[i, j] = 0\n",
    "\n",
    "# Make a 3D plot, do not change code\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', linewidth=0)\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can you judge your plot is correct?\n",
    "'''\n",
    "    Your Answer:\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
